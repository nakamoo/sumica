{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakamura/.pyenv/versions/anaconda3-4.4.0/envs/detection_server/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jupyter notebook --config=/home/nakamura/.jupyter/jupyter_notebook_config.py\n",
    "# %matplotlib nbagg\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"/home/nakamura/HAI/main_server/hai\")\n",
    "\n",
    "import coloredlogs, logging\n",
    "logger = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='CRITICAL', logger=logger)\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "from matplotlib import pylab as plt\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from bson.objectid import ObjectId\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from notebooks.utils.utils import visualize, display_latest_image, display_image, print_time, strtime_to_epoch, epoch_to_strtime, UpdateDist, display_two_images\n",
    "from controllers.actionrec import ActionRecognition\n",
    "from controllers.dbreader.hue_koki_dbreader import HueDBReader, pair_images, extract_color, ExprHueDBReader\n",
    "from controllers.dbreader.opedbreader import OperationDBReader, collect_img_op\n",
    "from controllers.dbreader.imagereader import ImageReader\n",
    "from controllers.vectorizer.posennvectorizer import PoseNNVectorizer\n",
    "from controllers.vectorizer.person2vec import Person2Vec\n",
    "\n",
    "mongo = MongoClient('localhost', 20202).hai\n",
    "\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "app.config.from_pyfile(filename=\"application.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from controllers.learner.i3dnn import I3DNN\n",
    "from .controller import Controller\n",
    "from controllers.utils import filter_persons\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "class ActionRecognition(Controller):\n",
    "    def __init__(self):\n",
    "        self.nn = I3DNN()\n",
    "\n",
    "    def on_event(self, event, data):\n",
    "        if event == \"image\":\n",
    "            if app.config['ENCRYPTION']:\n",
    "                image_path = app.config['ENCRYPTED_IMG_DIR'] + data['filename']\n",
    "            else:\n",
    "                image_path = app.config['RAW_IMG_DIR'] + data['filename']\n",
    "\n",
    "            n = mongo.images.find_one({\"_id\": data[\"_id\"]})\n",
    "            try:\n",
    "                persons, pose_indices = filter_persons(n)\n",
    "            except:\n",
    "                return\n",
    "            \n",
    "            img = cv2.imread(image_path)\n",
    "            whole_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 128.0 - 1.0\n",
    "            updates = {\"history.action_request\": time.time()}\n",
    "            \n",
    "            for i in range(len(n[\"detections\"])):\n",
    "                if n[\"detections\"][i][\"label\"] == \"person\":\n",
    "                    updates[\"detections.{}.passed\".format(i)] = i in persons\n",
    "                    if i in persons:\n",
    "                        box = n[\"detections\"][i][\"box\"]\n",
    "                        longer_side = max((box[2]-box[0])*2.0,(box[3]-box[1])*2.0)\n",
    "                        longer_side = min(min(whole_img.shape[1], longer_side), whole_img.shape[0])\n",
    "                            \n",
    "                        a = int(longer_side/2)\n",
    "                        cx, cy = (box[0]+box[2])//2, (box[1]+box[3])//2\n",
    "                        x1, y1, x2, y2 = cx-a, cy-a, cx+a, cy+a\n",
    "                        if x1 < 0:\n",
    "                            x2 -= x1\n",
    "                            x1 = 0\n",
    "                        if y1 < 0:\n",
    "                            y2 -= y1\n",
    "                            y1 = 0\n",
    "                        if x2 >= whole_img.shape[1]:\n",
    "                            x1 -= x2-whole_img.shape[1]\n",
    "                            x2 = whole_img.shape[1]\n",
    "                        if y2 >= whole_img.shape[0]:\n",
    "                            y1 -= y2-whole_img.shape[0]\n",
    "                            y2 = whole_img.shape[0]\n",
    "                            \n",
    "                        crop = whole_img[y1:y2,x1:x2,:]\n",
    "                        crop = cv2.resize(crop, (224, 224))\n",
    "                        img = np.array([[crop for _ in range(10)]])\n",
    "                        prob, logits, label, feats = self.nn.process_image(img)\n",
    "                        updates[\"detections.{}.action\".format(i)] = label\n",
    "                        updates[\"detections.{}.action_confidence\".format(i)] = float(prob)\n",
    "                        updates[\"detections.{}.action_crop\".format(i)] = [x1,y1,x2,y2]\n",
    "                        updates[\"detections.{}.action_vector\".format(i)] = feats\n",
    "                        \n",
    "                        a = persons.index(i)\n",
    "                        if pose_indices[a] is not None:\n",
    "                            updates[\"detections.{}.pose_body_index\".format(i)] = pose_indices[a]\n",
    "\n",
    "            updates[\"history.action_recorded\"] = time.time()\n",
    "            mongo.images.update_one({\"_id\": data[\"_id\"]}, {'$set': updates}, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../../../sean/kinetics-i3d/data/checkpoints/rgb_imagenet/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "action_rec = ActionRecognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = strtime_to_epoch(\"2017-12-19 23:45:00\")\n",
    "end = start + 30 \n",
    "# start = strtime_to_epoch(\"2017-12-19 23:45:00\")\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_cols = mongo.images.find({'time': {'$gt': start, '$lt': end}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_cols.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1513694700.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_col in imgs_cols:\n",
    "    action_rec.on_event('image', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
